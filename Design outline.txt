===========
Assumptions
===========
* Our primary focus is in the reliable storage and retrieval of data in the face of the physical world which may cause data loss.  Output space comsumption needs to be balanced against loss prevention, with a bias toward loss prevention where practical, even if it consumes a bit more output space.
* A secondary focus is for an open and portable format.  Ultimately, this should not be the only software capable of reading/writing this format.  Barcode formats we use must be unencumbered by patents.  Barcode formats should have widespread read and write support so others can more easily build software to read/write in our format.
* We're going to be taking in a general stream of bytes.  No format data will be provided by the user other than that it's a general octet stream.  We'll have a filename, but will only be including that for humans to read and not as part of the data.  The only thing in the data stream is the actual input data bytes.
* To restate, machine-readable data is not to be generated based on implied metadata like filenames.  If a user wants to give us a different document "title" to print for them, that's what we use.
* Encryption is outside of the scope of this program.  If encryption is needed, that should happen before the octet stream is passed to us.
* Compression is outside of the scope of this program.  If compression is needed, that should happen before the octet stream is passed to us.
* We do not support multiple files.  It is purely an octet stream, nothing more.  If a file structure is needed, an archive format like .tar, .bz2, or .zip could be used before being passed to us.  Anything further is outside of the scope of this program at this time.  We may want to add some redimentary support to the decoder since we might not need all of the pages to reconstruct one file from an encoded tarball.
* Open and well-known algorithms are to be preferred for easier implementation.  Ideally, it would be great if this could be implemented in other languages, for example so that a cell phone could decode the data from this if it had a good enough camera and the density was low enough.
* Encoding and decoding should be as multithreaded as possible.
* At some point, we may be adding encoding metadata to the output stream so it can be more easily decoded.  Currently, this is outside of the initial scope, but the format will include a version number to allow for future expansion.
* Each version number's format will be considered stable.  If a specific format version is targeted, it means the older software which supported it should still be able to read it, even if it's generated by newer software.
* Output must be reproducible for a program version - no randomness is to be used.  This means no salting of the document hash, no random document ID, nothing like that.  A given set of input and options in a given format version should only ever generate the exact same output as other runs when run using the same version of the program.  Pseudorandomness is allowed, but only using regenerable seed values such as page numbers and barcode numbers so they will generate exactly the same way in future runs.



===========
Data format
===========
Various barcode formats may be used.  Initially, we're going to try for QR codes, but Datamatrix or Aztec codes might be found to work as well.  Or we might want to build our own barcode format.  That said, structured append is a great idea in theory, but many encoders and decoders do not support it and it generally only supports up to 16 barcodes strung together.  In the interest of making encoders and decoders easy to write in other languages, we'll be doing that ourselves.

We would probably be better off with a high-density barcode format where a single barcode could cover an entire page and carry parity information for the whole thing within it.  That way, any section of the page could be damaged to a percentage without data loss, whereas with many smaller barcodes we're less immune to that.  But that is left as an exercise for future format versions, and only if readers and writers for this hypothetical barcode format were commonplace for easy portability.

Metadata fields are to remain a constant length across the entire document.  This way, number of pages can be properly estimated before starting.

Barcodes can be distributed around the page in any order, ideally at (pseudo-)random.  The barcode number can then be used to sort their data back into order.  Really, although this would be difficult to do with the damage likelihood map, they probably should be pseudo-randomized in a stable manner for best protection against problems in the same spot on multiple pages.

We would also need to figure out how to do parity correctly with pseudorandomized ordering.  It would make it much more durable to damage to the same places across the entire document (water damage that bleeds the ink out of one corner of all of the pages, for example) but it has the potential to complicate the parity process significantly and introduce far more unnecessary disk seeks to generate parity if we're not careful.

Data is to be padded out to fit the end of the final page, which is why we're included a total data length.  This padding is included in the parity calculations and can be anything which is stable across runs - zeroes or stable pseudorandom data.  For the initial encoder version, we may be using zeroes.  But even if we are, the reader cannot assume this and must read this data for the purposes of parity checking.

Parity is generated across the entire document.  The first parity byte is the parity of the first bytes of all pages.  The second parity byte is the parity of the second bytes of all pages.  Although exceedingly time-consuming to generate, and requiring tons of seeks across the source file, this means the error correction covers the entire file.  If we did it more segmented, loss could only occur a few times per segment.
Additional pages of parity are subsequent parity output for the exact same input bytes.  So the first parity byte of the second parity page protects the first byte of all of the other pages.  Again, exceedingly time-consuming and memory-intensive, but this allows for very good protection against losing entire pages.
Since we're doing this parity calculation essentially striped across the entire document, the final page data padding must be taken into account for parity calculations.

Barcodes should be able to be read independently of the rest of the document.  This means you should be able to tell where your page is in a document using only a fragment of a surviving page.  This wastes some space, but means we can put fewer constraints on the human side of the storage and scanning process and allow for more human error without data loss.

TODO: Figure out how to do on-page parity as well, so pages can contain both data and parity.  This would allow for short documents to be single-page and still include parity, and would also add an extra level of durability and validation against damage.

Each barcode is encoded as follows (nominally 20 bytes wasted per barcode when using version 1):
Format version: encoded similarly to UTF-8 - at this time, the high bit is reserved for this for later use and we can assume the version will be one byte for quite a few versions.
Page number: 16-bit big endian unsigned integer.  More compact than storing "bytes per page" and can be used to easily make user interfaces for partial scanning or recovering missing pages.  Total page counts include the parity pages.  Page numbers are 1-based so the data representation matches what's displayed to the user and optionally shown in printable page metadata.
Barcode number on page: 16-bit big endian unsigned integer/bitfield (required so we can recover from barcodes out of order or some barcodes unrecoverable):
* 16: Whether this is a parity page
* 15: (Not sure we actually need this.  Leaving unimplemented for now.  If not used in completed implementation, drop it.) Whether this is the last barcode in the regular data or parity sections.  For both sections, their last barcode should have this bit set.
* 14: (Not sure we actually need this.  Leaving unimplemented for now.  If not used in completed implementation, drop it.) Whether this is the last barcode in the page.
* 13: Reserved
* 12-1: Barcode number
Offset from start of file: 48-bit big endian unsigned integer indicating number of bytes offset from the start of the file that the start of this barcode's data is at.  Required because for proper seeking, we either need to know how many bytes per page and multiply, or have an offset.  This is nearly as compact and much more reliable than a page length and multiplier.  For data, this may be past the end of the document if we're in the padding section on the final page.  For parity pages, this number is meaningless, so instead it contains the index of the parity bytes (which parity page) we're encoding.
Total document length: 48-bit big endian unsigned integer indicating number of bytes in the original source document.  This field is the same for all barcodes, both data and parity.
Document hash: 24-bit unsigned integer.  To reduce the likelihood of mixing up pages from two documents of the same length.  Must be the same for all pages in our document.  Lowest three bytes of hash of the document, generated using a single-depth CRC32 Merkle tree with block size of 1MiB for better parallelism than straight CRC32.  There are more standardized ways to do this (some of the SHA-3 candidates, Tiger Tree Hash, etc.), but we don't need the security they provide, so CRC32 was chosen due to simplicity of implementation.  Essentially, take each 1MiB bucket of data (with the document padded with zeroes to a 1MiB boundary), generate a CRC32, concatenate all CRC32s together in big-endian format, and take the CRC32 of that.  Then take the 3 lowest bytes of that CRC32 (big endian order) to use for the hash.
Data chunk: octets of data until the end of the barcode.  Or, if filling out the final page with data, this can be padding which is included in parity calculations and to make barcode fitting on a page consistent across all pages.



===============
Encoder process
===============
We need to support both color and B&W output formats.  Color, for high data density on printers and scanners which can support it.  B&W for likely better fault-tolerance and for use with laser engravers and such, for truly long-term storage.
For color, palettes for the initial version of this software will need to be in a power-of-two length.  This is so we can interleave multiple B&W barcodes and decode them again using standardized decoders.  If we develop a custom barcode which supports colors natively, this restriction may be lifted in the future.


Damage likelihood map function
We need to have a constant one of these which always returns the same damage likelihood for any position on the page.  This can be used for output applications which are assumed to be equally likely to be damaged across the entire page, like data engraved in the center of a large object with lots of space around the outside.
This can be used to add extra error correction near the corners of pages (to account for staples or water damage) or for human-readable metadata near the top and bottom of the page.
Output needs to be stable from input - every given set of inputs must always return exactly the same output.  No randomness is to be allowed.
* Given an X, Y coordniate from (0.0-1.0), return a floating point estimation of the likelihood of damage, in a range of 0..1 with 0 being "don't worry about damage" and 1 being "maximum likelihood of damage relative to the document" i.e. it may still not be very likely, but out of everywhere in the document, it's the most likely to be damaged and should use the most error correction here of anywhere.  But this does not necessarily mean "use the maximum error correction available for the barcode type being used" - the user may specify something lower than the theoretical maximum for greater storage density.

PageBarcodePacker
(Restriction that all barcodes must encode at a constant data rate per page, which allows us to allocate parity much easier)
Represents only the data area of a page - where the barcodes are actually stored - not including metadata which is added at higher levels.  Responsible for fitting as many barcodes into the output image as possible (including quiet zones) and encoding information to be able to reconstruct them if the barcodes are scanned out of order.
* Given an output image size, in pixels
* Given a barcoding system (QR code, Datamatrix, etc.)
* Optionally given a color palette (default of black and white) to use to either multiplex binary barcodes.  Must be an array of >=2 elements with a length that's a power of two
* Optionally given a damage likelihood map function
* Optionally given the output format version number, for compatibility with older software.  If not given, default to the latest supported version.
* Returns how much data this page can hold
* Can be given a page number, total number of pages in document, boolean indicating whether this is the last page in a data segment or parity segment, an integer indicating which page of parity it is if it is a parity page so that it can be put into the offset, document checksum (see barcode data format) and data of the maximum size or less, which returns an image which fits the parameters
* Can be asked to re-optimize for a different minimum per-page length, so on the off-chance we can still fit as much data as we need at a larger barcode size, we can do that.

InputFileReader
A reader for a given input file.
* Given an input filename.
* Can be asked for the total length of the file.
* Can be asked for a hash of the entire file.  Generated using a single-depth CRC32 Merkle tree with block size of 1MiB (input file padded with zeroes to 1MiB boundary) for better parallelism than straight CRC32, at least in the future.  Essentially, take each 1MiB bucket of data, generate a CRC32, concatenate all CRC32s together, and take the CRC32 of that.  Returns the full CRC32, even if we're only going to use the first three bytes in the barcodes.
* Can be asked for chunks of this file.  Fills as much of a provided buffer as possible and returns the actual number of bytes read.  Does not return a checksum, because it may need to be padded to an arbitrary length outside of this function, depending on damage likelihood map.

ArchiveHumanOutputFile
A specific format for output.  We'll need one of these (eventually) for: PNG, EPS, PDF, and SVG.  Initial version to only include PNG since it's really easy.
* Given a page size
* Given a list of metadata to include in the output file (what to output and where), in some kind of templated format so the page number from PageOrganizer can be added in
* Provides a callback for PageOrganizer to use to give us a page, which will not necessarily be in the correct order due to multithreading.  The callback will need to require the page number.

PageOrganizer
Organizes data into multiple pages, padding as needed, and splitting the data into page-sized chunks with page numbers to be able to reconstruct multiple pages if scanned out of order.
* Given an InputFileReader to read from.
* Given an ArchiveHumanOutputFile to use for generating pages
* Given a PageBarcodePacker to use to fill the barcode area with barcodes
* Optionally given a number of additional parity pages to use to correct for lost pages
* Optionally given a number of concurrent encoding threads to use.
* Can be asked how many pages a given amount of data will take
* Can be given data to encode and a number of threads, which requires a callback to use to save them so it can be paralellized and doesn't have to allocate RAM for the entire job at once

ArchiveHumanFormatter
Adds human-readable metadata to the page so it can be archived and organized safely.  Also adds in the damage likelihood map.
* Given which printable output format we should use (not the actual struct, since we'll need to construct that with all of the metadata in here)
* Given an output file/folder (depending on format)
* Given a PageOrganizer, which gives us access to the input file, the metadata for the page, as well as the PageBarcodePacker with the color palette and damage likelihood map
* Has a function to initiate output, which passes a callback to PageOrganizer to write pages to.  This struct then assembles that output into an output file.

StressTestPage
Generates a page to stress test the capabilities of the printer and scanner to show error rates at different settings.  Use case is for the user to generate one, print it, damage it if they want, scan it, and show error rates for different options.
* Given an output image size
* Given a barcoding system
* Given the DPI range to test
* Optionally given a color palette (default of black and white) to use to either multiplex binary barcodes
* Returns an image with a test barcode for different DPI densities and both using the color palette and not, with each barcode encoding a text description of what the format is so successful reads can be easily displayed to the user.  Given a callback to call when generation of the image is complete.



===============
Decoder process
===============
1. Sparse output file is created if it doesn't exist.  If it does exist and we have an error log file which also exists, assume we're trying to recover bad spots.
2. Decoder takes in images from files and dispatches them to threads, which each write to a portion of the output file.  If recovering from a previous failed run, once a page's metadata has been read, if it was already read successfully, ignore it.
3. If asked of us, warnings are logged to show where corrupted data exists in the read.  Data will be logged per page, not per barcode.  Error rate percentage per page.
4. If asked of us, errors are logged to show unrecoverable sections of the file.  If the error log already exists, assume we are rerunning on new data and delete errors from the output file once they are successfully recovered.