===========
Assumptions
===========
* Our primary focus is in the reliable storage and retrieval of data in the face of the physical world which may cause data loss.  Output space comsumption needs to be balanced against loss prevention, with a bias toward loss prevention where practical, even if it consumes a bit more output space.
* A secondary focus is for an open and portable format.  Ultimately, this should not be the only software capable of reading/writing this format.  Barcode formats we use must be unencumbered by patents.  Barcode formats should have widespread read and write support so others can more easily build software to read/write in our format.
* We're going to be taking in a general stream of bytes.  No format data will be provided by the user other than that it's a general octet stream.  We'll have a filename, but will only be including that for humans to read and not as part of the data.  The only thing in the data stream is the actual input data bytes.
* To restate, machine-readable data is not to be generated based on implied metadata like filenames.  If a user wants to give us a different document "title" to print for them, that's what we use.
* Encryption is outside of the scope of this program.  If encryption is needed, that should happen before the octet stream is passed to us.
* Compression is outside of the scope of this program.  If compression is needed, that should happen before the octet stream is passed to us.
* We do not support multiple files.  It is purely an octet stream, nothing more.  If a file structure is needed, an archive format like .tar, .bz2, or .zip could be used before being passed to us.  Anything further is outside of the scope of this program at this time.  We may want to add some redimentary support to the decoder since we might not need all of the pages to reconstruct one file from an encoded tarball.
* Open and well-known algorithms are to be preferred for easier implementation.  Ideally, it would be great if this could be implemented in other languages, for example so that a cell phone could decode the data from this if it had a good enough camera and the density was low enough.
* Encoding and decoding should be as multithreaded as possible.
* At some point, we may be adding encoding metadata to the output stream so it can be more easily decoded.  Currently, this is outside of the initial scope, but the format will include a version number to allow for future expansion.
* Each version number's format will be considered stable.  If a specific format version is targeted, it means the older software which supported it should still be able to read it, even if it's generated by newer software.
* Output must be reproducible for a program version - no randomness is to be used.  This means no salting of the document hash, no random document ID, nothing like that.  A given set of input and options in a given format version should only ever generate the exact same output as other runs when run using the same version of the program.  Pseudorandomness is allowed, but only using regenerable seed values such as page numbers and barcode numbers so they will generate exactly the same way in future runs.
* When using barcodes which are multiplexed over multiple colors, the individual monochrome barcodes which comprise the multi-color barcode are to be encoded as standalone barcoded data chunks.  This does waste a bit of data rate on the individual barcodes' metadata, but it does mean that data is a little more resilient because damage to one color doesn't necessarily mess up the entire data chunk.  It also means that we can read it by using off-the-shelf QR decoders by first doing color separation on the entire page and then doing barcode recognition in monochrome, which should make porting this system to other programming languages or mobile readers easier.  The alternative would be to have to build a recognizer for the multi-color barcodes, then demultiplex them, then feed them to an off-the-shelf decoder, which seems unnecessarily complicated.



======
Limits
======
* Files which can encoded are limited to a size of 2 ^ 48 bytes (256 TiB).  This is due to the 48-bit unsigned start offsets for barcodes.
* A document can only contain 2 ^ 16 - 1 (65535) pages.  This is because page numbers are 1-based for human readability and are stored in a 16-bit unsigned page number value.
* A page can only contain 2 ^ 12 (4096) "barcodes".  This is because barcode numbers are stored in the lower 12 bits of a 16-bit unsigned value.  This includes the color planes being multiplexed into a section of the page, so for color documents you will need to divide this number by the number of available color plane bits.
* Number of parity pages can be any number from 0-255.  This is because the parity index is stored as a single byte.
* If parity is used, each page can only contain 2 ^ 32 bytes (4 GiB).  This is because the start offset for parity is stored as a 32-bit unsigned value.  Theoretically, this could be dropped in the future if we can figure out an intelligent way to find the start index of a parity chunk by finding a matching barcode number somewhere in the document and stepping backward by the page size until we reach the start.  However, for practical purposes, this limit is unlikely to be hit in practice.



===========
Data format
===========
Various barcode formats may be used.  Initially, we're going to try for QR codes, but Datamatrix or Aztec codes might be found to work as well.  Or we might want to build our own barcode format.  That said, structured append is a great idea in theory, but many encoders and decoders do not support it and it generally only supports up to 16 barcodes strung together.  In the interest of making encoders and decoders easy to write in other languages, we'll be doing that ourselves.

We would probably be better off with a high-density barcode format where a single barcode could cover an entire page and carry parity information for the whole thing within it.  That way, any section of the page could be damaged to a percentage without data loss, whereas with many smaller barcodes we're less immune to that.  But that is left as an exercise for future format versions, and only if readers and writers for this hypothetical barcode format were commonplace for easy portability.

Metadata fields are to remain a constant length across the entire document.  This way, number of pages can be properly estimated before starting.

Barcodes can be distributed around the page in any order, ideally at (pseudo-)random.  The barcode number or start offset position can then be used to sort their data back into order.  Really, although this would be difficult to do with the damage likelihood map, they probably should be pseudo-randomized in a stable manner using the page number as a seed for best protection against problems in the same spot on multiple pages.

We would also need to figure out how to do parity correctly with pseudorandomized ordering.  It would make it much more durable to damage to the same places across the entire document (water damage that bleeds the ink out of one corner of all of the pages, for example) but it has the potential to complicate the parity process significantly and introduce far more unnecessary disk seeks to generate parity if we're not careful.  Possibility limit the order randomization step to only reordering barcodes within the same damage resiliency class and thus the same data length.  Then parity over all of the "page X, code 0" would still be able to be the same length so we can do Reed-Solomon without having to then restripe the parity over an entire page.

Data is to be padded out to fit the end of the final page, which is why we're included a total data length.  This padding is included in the parity calculations and can be anything which is stable across runs - zeroes or stable pseudorandom data.  For the initial encoder version, we may be using zeroes.  But even if we are, the reader cannot assume this and must read this data for the purposes of parity checking.

Parity is generated across the entire document.  The first parity byte (one with starting offset 0 and skip of a page length) is the parity of the first bytes of all pages concatenated together.  The second parity byte is the parity of the second bytes of all pages.  Although exceedingly time-consuming to generate, and requiring tons of seeks across the source file, this means the error correction covers the entire file.  If we did it more segmented, loss could only occur a few times per segment.
Additional pages of parity are subsequent parity output for the exact same input bytes.  So the first parity byte of the second parity page protects the first byte of all of the other pages.  Again, exceedingly time-consuming and memory-intensive, but this allows for very good protection against losing entire pages.
Since we're doing this parity calculation essentially striped across the entire document, the final page data padding must be taken into account for parity calculations.

Barcodes should be able to be read independently of the rest of the document.  This means you should be able to tell where your page is in a document using only a fragment of a surviving page.  This wastes some space, but means we can put fewer constraints on the human side of the storage and scanning process and allow for more human error without data loss.

TODO: Figure out how to do on-page parity as well, so pages can contain both data and parity.  This would allow for short documents to be single-page and still include parity, and would also add an extra level of durability and validation against damage.

Barcode binary data is encoded using base-45 encoding (see IETF RFC 9385) so we can use alphanumeric mode in our barcodes.  This may come at a slight density cost, but encoding/decoding support for raw binary mode is not great.

Within the base-45 encoding, the binary data for each barcode is encoded as follows (nominally 20 bytes wasted per barcode when using version 1):
* Format version: encoded similarly to UTF-8 - at this time, the high bit is reserved for this for later use and we can assume the version will be one byte for quite a few versions.
* Page number: 16-bit big endian unsigned integer.  More compact than storing "bytes per page" and can be used to easily make user interfaces for partial scanning or recovering missing pages.  Total page counts include the parity pages. Page numbers are 1-based so the data representation matches what's displayed to the user and optionally shown in printable page metadata.  We don't really need this for reconstruction - from a functional perspective, it's purely so we can show the user which pages we've read and can keep track of which ones are missing.
* Barcode number on page: 16-bit big endian unsigned integer/bitfield (required so we can recover from barcodes out of order or some barcodes unrecoverable):
* - 16: Whether this is a parity barcode
* - 15: (Not sure we actually need this.  Leaving unimplemented for now.  If not used in completed implementation, drop it.) Whether this is the last barcode in the regular data or parity sections.  For both sections, their last barcode should have this bit set.
* - 14: (Not sure we actually need this.  Leaving unimplemented for now.  If not used in completed implementation, drop it.) Whether this is the last barcode in the page.
* - 13: Reserved
* - 12-1: Barcode number
* Offset from start of file:
* - For data barcodes, 48-bit big endian unsigned integer indicating number of bytes offset from the start of the file that the start of this barcode's data is at.  Required because for proper seeking, we either need to know how many bytes per page and multiply, or have an offset.  This is nearly as compact and much more reliable than a page length and multiplier.  For data, this may be past the end of the document if we're in the padding section on the final page.  
* - For parity pages, this number is meaningless, so instead it contains (apologies for the weird alignment, but parity is likely to be a very small proportion of a document that it's not worth reducing data density for it):
* - - 1 byte reserved for future use.  Set to 0 for now.
* - - 1 byte of the index of the parity bytes (which parity page) we're encoding.
* - - 4 bytes indicating where the parity starts.  Parity is defined using a start offset and a skip factor.  So for example, for barcode 0 this would also be 0 to indicate that this parity block starts protecting data startiong at byte 0.  Skip factor must be calculated by comparing matching barcodes between pages - for example, the offset difference between barcode 0 of the first page to barcode 0 of the second page.
* Total document length: 48-bit big endian unsigned integer indicating number of bytes in the original source document.  This field is the same for all barcodes, both data and parity.  This count does not include the length of the parity data, but is only the length of the original file.
* Document hash: 24-bit unsigned integer.  To reduce the likelihood of mixing up pages from two documents of the same length.  Must be the same for all pages in our document.  Lowest three bytes of hash of the document, generated using a single-depth CRC32 Merkle tree with block size of 1MiB for better parallelism than straight CRC32.  There are more standardized ways to do this (some of the SHA-3 candidates, Tiger Tree Hash, etc.), but we don't need the security they provide, so CRC32 was chosen due to simplicity of implementation.  Essentially, take each 1MiB bucket of data (with the document padded with zeroes to a 1MiB boundary), generate a CRC32, concatenate all CRC32s together in big-endian format, and take the CRC32 of that.  Then take the 3 lowest bytes of that CRC32 (big endian order) to use for the hash.
* Data chunk: octets of data until the end of the barcode.  Or, if filling out the final page with data, this can be padding which is included in parity calculations and to make barcode fitting on a page consistent across all pages.


==================
Color multiplexing
==================
For a given set of colors, a palette would be generated which spaces the colors evenly (or nearly so) over the hue space (in an HSV color model), with white and black both included.  The palette is then rearranged in Gray code order - the idea being that when scanned, if the hue is quantized into an adjoining hue bucket, it will ideally only cause a bit flip in one of the output bit planes.  The exception to this order is that white and black need to be mapped to all 1 and all 0, respectively, so the location features of QR codes are still visible as monochrome.  Black should map correctly regardless if it's at position 0, white needs to be dealt with separately, as detailed in the example below, to ensure that it is always the last color in the palette.  Planes would be encoded in this by iterating pixel-by-pixel, combining the planes together into a number, and then using the Gray-coded palette to generate a color output pixel.  Conversely, when decoding, the image is first quantized into the hue palette and then decoded into monochrome pixels using the bits in the palette index.  So, for an example with 8 colors, the color palette would be generated as as follows with white being excluded at this step:

0 = 0 deg/0%/0% HSL (Black)
1 = 0 deg/100%/100% HSL (Red)
2 = 60 deg/100%/50% HSL (Yellow)
3 = 120 deg/100%/50% HSL (Green)
4 = 180 deg/100%/50% HSL (Cyan)
5 = 240 deg/100%/50% HSL (Blue)
6 = 300 deg/100%/50% HSL (Fuschia)
7 = Placeholder - excluded from generating for now

Then colors are rearranged based on their Gray code mapping:

0 = Gray code 000 = binary 000 (0) = 0 deg/0%/0% HSL (Black)
1 = Gray code 001 = binary 001 (1) = 0 deg/100%/50% HSV (Red)
2 = Gray code 010 = binary 011 (3) = 120 deg/100%/50% HSL (Green)
3 = Gray code 011 = binary 010 (2) = 60 deg/100%/50% HSL (Yellow)
4 = Gray code 100 = binary 111 (7) = Placeholder
5 = Gray code 101 = binary 110 (6) = 300 deg/100%/50% HSL (Fuschia)
6 = Gray code 110 = binary 100 (4) = 180 deg/100%/50% HSL (Cyan)
7 = Gray code 111 = binary 101 (5) = 240 deg/100%/50% HSL (Blue)

The placeholder is removed and white added to the end:

0 = Planes 000 = 0 deg/0%/0% HSL (Black)
1 = Planes 001 = 0 deg/100%/50% HSL (Red)
2 = Planes 010 = 120 deg/100%/50% HSL (Green)
3 = Planes 011 = 60 deg/100%/50% HSL (Yellow)
4 = Planes 100 = 300 deg/100%/50% HSL (Fuschia)
5 = Planes 101 = 180 deg/100%/50% HSL (Cyan)
6 = Planes 110 = 240 deg/100%/50% HSL (Blue)
7 = Planes 111 = 0 deg/0%/100% HSL (White)

So, for example, if a pixel was printed as blue but scanned and quantized as fuschia, we only have one bit plane which is flipped.  In this example, cyan and blue are somewhat problematic because of the shift for white.  However, the idea is that when generalized to an arbitrary power-of-two palette length, incorrect quantization should more often than not only result in a single bit being corrupted, and since it's unlikely that every color combination will be as easily corrupted, single bit flips are relatively likely to be corrected by the error correction inherent in the barcodes themselves.



===============
Encoder process
===============
We need to support both color and B&W output formats.  Color, for high data density on printers and scanners which can support it.  B&W for likely better fault-tolerance and for use with laser engravers and such, for truly long-term storage.
For color, palettes for the initial version of this software will need to be in a power-of-two length.  This is so we can interleave multiple B&W barcodes and decode them again using standardized decoders.  If we develop a custom barcode which supports colors natively, this restriction may be lifted in the future.

Color map
The idea here is that all color encoding and decoding would happen within the color map.
* Constructed with the number of colors desired.
* Can be asked how many planes of barcodes can be encoded.
* Can be given a vector of monochrome images and multiplexes them into an output image.
* If given an image, can be asked to demultiplex it into a vector of images where each one contains one monochrome plane.

Damage likelihood map function
We need to have a constant one of these which always returns the same damage likelihood for any position on the page.  This can be used for output applications which are assumed to be equally likely to be damaged across the entire page, like data engraved in the center of a large object with lots of space around the outside.
This can be used to add extra error correction near the corners of pages (to account for staples or water damage) or for human-readable metadata near the top and bottom of the page.
Output needs to be stable from input - every given set of inputs must always return exactly the same output.  No randomness is to be allowed.
* Given an X, Y coordniate from (0.0-1.0), return a floating point estimation of the likelihood of damage, in a range of 0..1 with 0 being "don't worry about damage" and 1 being "maximum likelihood of damage relative to the document" i.e. it may still not be very likely, but out of everywhere in the document, it's the most likely to be damaged and should use the most error correction here of anywhere.  But this does not necessarily mean "use the maximum error correction available for the barcode type being used" - the user may specify something lower than the theoretical maximum for greater storage density.

PageBarcodePacker
(Restriction that all barcodes must encode at a constant data rate per page, which allows us to allocate parity much easier)
Represents only the data area of a page - where the barcodes are actually stored - not including metadata which is added at higher levels.  Responsible for fitting as many barcodes into the output image as possible (including quiet zones) and encoding information to be able to reconstruct them if the barcodes are scanned out of order.  Ideally, when encoding, the barcodes being fed into the color map should be pseudorandomized to reduce the damage caused by hue being detected incorrectly.
* Given an output image size, in pixels
* Given a barcoding system (QR code, Datamatrix, etc.)
* Optionally given a color map - defaults to a color map of two (monochrome)
* Optionally given a damage likelihood map function
* Optionally given the output format version number, for compatibility with older software.  If not given, default to the latest supported version.
* Returns how much data this page can hold
* Can be given a page number, total number of pages in document, boolean indicating whether this is the last page in a data segment or parity segment, an integer indicating which page of parity it is if it is a parity page so that it can be put into the offset, document checksum (see barcode data format) and data of the maximum size or less, which returns an image which fits the parameters
* Can be asked to re-optimize for a different minimum per-page length, so on the off-chance we can still fit as much data as we need at a larger barcode size, we can do that.

InputFileReader
A reader for a given input file.
* Given an input filename.
* Can be asked for the total length of the file.
* Can be asked for a hash of the entire file.  Generated using a single-depth CRC32 Merkle tree with block size of 1MiB (input file padded with zeroes to 1MiB boundary) for better parallelism than straight CRC32, at least in the future.  Essentially, take each 1MiB bucket of data, generate a CRC32, concatenate all CRC32s together, and take the CRC32 of that.  Returns the full CRC32, even if we're only going to use the first three bytes in the barcodes.
* Can be asked for chunks of this file.  Fills as much of a provided buffer as possible and returns the actual number of bytes read.  Does not return a checksum, because it may need to be padded to an arbitrary length outside of this function, depending on damage likelihood map.

ArchiveHumanOutputFile
A specific format for output.  We'll need one of these (eventually) for: PNG, EPS, PDF, and SVG.  Initial version to only include PNG since it's really easy.
* Given a page size
* Given a list of metadata to include in the output file (what to output and where), in some kind of templated format so the page number from PageOrganizer can be added in
* Provides a callback for PageOrganizer to use to give us a page, which will not necessarily be in the correct order due to multithreading.  The callback will need to require the page number.

PageOrganizer
Organizes data into multiple pages, padding as needed, and splitting the data into page-sized chunks with page numbers to be able to reconstruct multiple pages if scanned out of order.
* Given an InputFileReader to read from.
* Given an ArchiveHumanOutputFile to use for generating pages
* Given a PageBarcodePacker to use to fill the barcode area with barcodes
* Optionally given a number of additional parity pages to use to correct for lost pages
* Optionally given a number of concurrent encoding threads to use.
* Can be asked how many pages a given amount of data will take
* Can be given data to encode and a number of threads, which requires a callback to use to save them so it can be paralellized and doesn't have to allocate RAM for the entire job at once

ArchiveHumanFormatter
Adds human-readable metadata to the page so it can be archived and organized safely.  Also adds in the damage likelihood map.
* Given which printable output format we should use (not the actual struct, since we'll need to construct that with all of the metadata in here)
* Given an output file/folder (depending on format)
* Given a PageOrganizer, which gives us access to the input file, the metadata for the page, as well as the PageBarcodePacker with the color palette and damage likelihood map
* Has a function to initiate output, which passes a callback to PageOrganizer to write pages to.  This struct then assembles that output into an output file.

StressTestPage
Generates a page to stress test the capabilities of the printer and scanner to show error rates at different settings.  Use case is for the user to generate one, print it, damage it if they want, scan it, and show error rates for different options.
* Given an output image size
* Given a barcoding system
* Given the DPI range to test
* Optionally given a color palette (default of black and white) to use to either multiplex binary barcodes
* Returns an image with a test barcode for different DPI densities and both using the color palette and not, with each barcode encoding a text description of what the format is so successful reads can be easily displayed to the user.  Given a callback to call when generation of the image is complete.



===============
Decoder process
===============
1. Sparse output file is created if it doesn't exist.  If it does exist and we have an error log file which also exists, assume we're trying to recover bad spots.
2. Decoder takes in images from files and dispatches them to threads, which each write to a portion of the output file.  If recovering from a previous failed run, once a page's metadata has been read, if it was already read successfully, ignore it.
3. If asked of us, warnings are logged to show where corrupted data exists in the read.  Data will be logged per page, not per barcode.  Error rate percentage per page.
4. If asked of us, errors are logged to show unrecoverable sections of the file.  If the error log already exists, assume we are rerunning on new data and delete errors from the output file once they are successfully recovered.